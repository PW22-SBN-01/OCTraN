<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <!-- ======================================================================= -->
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <style type="text/css">
      body {
      font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
      font-weight:300;
      font-size:18px;
      margin-left: auto;
      margin-right: auto;
      width: 1100px;
      }
      h1 {
      font-weight:300;
      }
      .disclaimerbox {
      background-color: #eee;
      border: 1px solid #eeeeee;
      border-radius: 10px ;
      -moz-border-radius: 10px ;
      -webkit-border-radius: 10px ;
      padding: 20px;
      }
      video.header-vid {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px ;
      -moz-border-radius: 10px ;
      -webkit-border-radius: 10px ;
      }
      img.header-img {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px ;
      -moz-border-radius: 10px ;
      -webkit-border-radius: 10px ;
      }
      img.rounded {
      border: 1px solid #eeeeee;
      border-radius: 10px ;
      -moz-border-radius: 10px ;
      -webkit-border-radius: 10px ;
      }
      a:link,a:visited
      {
      color: #1367a7;
      text-decoration: none;
      }
      a:hover {
      color: #208799;
      }
      td.dl-link {
      height: 160px;
      text-align: center;
      font-size: 22px;
      }
      .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
      0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
      5px 5px 0 0px #fff, /* The second layer */
      5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
      10px 10px 0 0px #fff, /* The third layer */
      10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
      15px 15px 0 0px #fff, /* The fourth layer */
      15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
      20px 20px 0 0px #fff, /* The fifth layer */
      20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
      25px 25px 0 0px #fff, /* The fifth layer */
      25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
      margin-left: 10px;
      margin-right: 45px;
      }
      .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
      0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
      5px 5px 0 0px #fff, /* The second layer */
      5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
      10px 10px 0 0px #fff, /* The third layer */
      10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
      margin-top: 5px;
      margin-left: 10px;
      margin-right: 30px;
      margin-bottom: 5px;
      }
      .vert-cent {
      position: relative;
      top: 50%;
      transform: translateY(-50%);
      }
      hr
      {
      border: 0;
      height: 1px;
      background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
      }
      #authors td {
      padding-bottom:5px;
      padding-top:30px;
      }
    </style>
    <!-- ======================================================================= -->
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      
      gtag('config', 'UA-114291442-5');
    </script>
    <script type="text/javascript" src="./index_files/hidebib.js"></script>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>OCTraN</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
  </head>
  <body class="vsc-initialized">
    <br>
    <center><span style="font-size:44px;font-weight:bold;">OCTraN: 3D Occupancy Convolutional Transformer Network in Unstructured Traffic Scenarios</span></center>
    <br>
    <table align="center" width="710px">
      <tbody>
        <tr>
          <td align="center" width="230px">
            <center><span style="font-size:22px"><a href="https://www.linkedin.com/in/adityang/" target="_blank">Aditya N G</a></span></center>
          </td>
          <td align="center" width="230px">
            <center><span style="font-size:22px"><a href="https://in.linkedin.com/in/dhruvalpb" target="_blank">Dhruval P B</a></span></center>
          </td>
          <td align="center" width="230px">
            <center><span style="font-size:22px"><a href="https://scholar.google.com/citations?user=7hBzcg0AAAAJ&hl=en" target="_blank">Harshith M K</a></span></center>
          </td>
          <td align="center" width="230px">
            <center><span style="font-size:22px"><a href="https://www.linkedin.com/in/priyaasuresh/?originalSubdomain=in" target="_blank">Priya S S</a></span></center>
          </td>
          <td align="center" width="230px">
            <center><span style="font-size:22px"><a href="https://staff.pes.edu/nm1306/" target="_blank">Dr. Surabhi Narayan</a></span></center>
          </td>
        </tr>

        <tr>
          <td align="center" width="230px">
            <center><span style="font-size:11px"><a href="https://www.linkedin.com/in/adityang/" target="_blank">adityang5@gmail.com</a></span></center>
          </td>
          <td align="center" width="230px">
            <center><span style="font-size:11px"><a href="https://in.linkedin.com/in/dhruvalpb" target="_blank">adityang5@gmail.com</a></span></center>
          </td>
          <td align="center" width="230px">
            <center><span style="font-size:11px"><a href="https://scholar.google.com/citations?user=7hBzcg0AAAAJ&hl=en" target="_blank">adityang5@gmail.com</a></span></center>
          </td>
          <td align="center" width="230px">
            <center><span style="font-size:11px"><a href="https://www.linkedin.com/in/priyaasuresh/?originalSubdomain=in" target="_blank">adityang5@gmail.com</a></span></center>
          </td>
          <td align="center" width="230px">
            <center><span style="font-size:11px"><a href="https://staff.pes.edu/nm1306/" target="_blank">adityang5@gmail.com</a></span></center>
          </td>
        </tr>
        
      </tbody>
    </table>
    <br />
    <table align="center" width="700px">
      <tbody>
        <tr>
          <td align="center" width="230px">
            <center><span style="font-size:16px">Department of Computer Science, PES University, Bengaluru</span></center>
          </td>
        </tr>
        <tr>
          <td align="center" width="700px">
            <center><span style="font-size:16px">Spotlight presentation at <a href="https://sites.google.com/view/t4v-cvpr23/papers?authuser=0#:~:text=%5D%20%5Bvideo%5D%20%5Bsupplementary%5D-,OCTraN,-%3A%203D%20Occupancy%20Convolutional">CVPR's T4V 2023</a> </span></center>
          </td>
        </tr>
        <tr>
        </tr>
      </tbody>
    </table>
    <br>
    <table align="center" width="700px">
      <tbody>
        <tr>
          <td align="center" width="100px">
            <center><span style="font-size:28px"><a href="https://arxiv.org/abs/2307.10934">[Paper]</a></span></center>
          </td>
          <td align="center" width="100px">
            <center><span style="font-size:28px"><a href="https://www.youtube.com/watch?v=Llro9RwpVpE&ab_channel=AdityaNG">[Slides]</a></span></center>
          </td>
          <td align="center" width="100px">
            <center><span style="font-size:28px"><a href="https://github.com/PW22-SBN-01/OCTraN">[Code]</a></span></center>
          </td>
        </tr>
        <tr>
        </tr>
      </tbody>
    </table>
    <br>
    <table align="center" width="300px">
      <tbody>
        <tr>
          <td align="center" width="300px">
            <center><a href="./index_files/demo.gif">

              <tbody>
                <tr>
                  <td align="center" width="200px">
                    <center>
                      <span style="font-size:28px">
                        <img src="./index_files/OCTran_img_1.png" width="100%">
                      </span>
                    </center>
                  </td>
                </tr>
                <tr>
                  <td align="center" width="200px">
                    <center>
                      <span style="font-size:28px">
                        <img src="./index_files/OCTraN_merged_grid_2_rev.png" width="100%">
                      </span>
                    </center>
                  </td>
                </tr>
              </tbody>


            </a><br></center>
          </td>
        </tr>
      </tbody>
    </table>
    <br>
    <div style="width:800px; margin:0 auto; text-align:justify">Modern approaches for vision-centric environment perception for autonomous navigation make extensive use of self-supervised monocular depth estimation algorithms to predict the depth of a 3D scene using only a single camera image. However, when this depth map is projected onto 3D space, the errors in disparity are magnified,  resulting in a depth estimation error that increases quadratically as the distance from the camera increases. Though Light Detection and Ranging (LiDAR) can solve this issue, it is expensive and not feasible for many applications. To address the challenge of accurate ranging with low-cost sensors, we propose a transformer architecture that uses iterative-attention to convert 2D image features into 3D occupancy features and makes use of convolution and transpose convolution to efficiently operate on spatial information. We also develop a self-supervised training pipeline to generalize the model to any scene by eliminating the need for LiDAR ground truth by substituting it with pseudo-ground truth labels obtained from neural radiance fields (NeRFs) and boosting monocular depth estimation. We will be making our code and dataset public
    </div>
    <br>
    <hr>
    <center>
      <h1>Bengaluru Driving Dataset</h1>
    </center>
    <div style="width:800px; margin:0 auto; text-align:justify">We gathered a dataset spanning 114 minutes and 165K frames in Bengaluru, India. Our dataset consists of video data from a calibrated camera sensor with a resolution of 1920Ã—1080 recorded at a framerate of 30 Hz. We utilize a Depth Dataset Generation pipeline that only uses videos as input to produce high-resolution disparity maps</div>
    <br>
    <p style="margin-top:4px;"></p>
    <table align="center" width="1000px">
      <tbody>
        <tr>
          <td width="1200px">
            <center><a href="./index_files/plot_dataset_3D.png"><img src="./index_files/plot_dataset_3D.png" width="800px"></a><br></center>
          </td>
        </tr>
      </tbody>
    </table>
    <br>
    <hr>
    <center>
      <h1>Short Presentation</h1>
    </center>
    <table align="center" width="300px">
      <tbody>
        <tr>
          <td align="center" width="300px">
            <iframe width="768" height="480" src="https://www.youtube.com/embed/Llro9RwpVpE" title="OCTraN" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </td>
        </tr>
      </tbody>
    </table>
    <br>
    <hr>
    <center id="sourceCode">
      <h1>Source Code</h1>
    </center>
    <div style="width:800px; margin:0 auto; text-align=center">
      We have released the PyTorch implementation of OCTraN on GitHub. Try our code!
    </div>
    <table align="center" width="900px">
      <tbody>
        <tr>
          <td width="300px" align="center">
            <span style="font-size:28px"><a href="https://github.com/PW22-SBN-01/OCTraN">[GitHub]</a></span>
          </td>
        </tr>
      </tbody>
    </table>
    <br>
    <hr>
    <center>
      <h1>Paper and Bibtex</h1>
    </center>
    <table align="center" width="850px">
      <tbody>
        <tr>
          <td width="200px" align="left">
            <a href="https://github.com/PW22-SBN-01/OCTraN"><img style="width:400px" src="./index_files/paper_OCTraN.png"></a>
            <center>
              <span style="font-size:20pt">
                <a href="https://github.com/PW22-SBN-01/OCTraN">[Paper]</a>
              </span>
            </center>
          </td>
          <td width="50px" align="center">
          </td>
          <td width="550px" align="left">
            <p style="text-align:left;"><b><span style="font-size:20pt">Citation</span></b><br><span style="font-size:6px;">&nbsp;<br></span> <span style="font-size:15pt">Aditya N Ganesh, Dhruval Pobbathi Badrinath, Harshith Mohan Kumar, Priya S, and Surabhi Narayan. Octran: 3d occupancy convolutional transformer network in unstructured traffic scenarios. Spotlight Presentation at the Transformers for Vision Workshop, CVPR, 2023. Transformers for Vision Workshop, CVPR 2023</span></p>
            <span style="font-size:20pt"><a shape="rect" href="javascript:togglebib(&#39;assemblies19_bib&#39;)" class="togglebib">[Bibtex]</a></span>
            <div class="paper" id="assemblies19_bib">
              <pre xml:space="preserve" style="display: none;">
@misc{analgund2023octran,
  title={OCTraN: 3D Occupancy Convolutional Transformer Network in Unstructured Traffic Scenarios},
  author={Ganesh, Aditya N and Pobbathi Badrinath, Dhruval and
    Kumar, Harshith Mohan and S, Priya and Narayan, Surabhi
  },
  year={2023},
  howpublished={Spotlight Presentation at the Transformers for Vision Workshop, CVPR},
  url={https://sites.google.com/view/t4v-cvpr23/papers#h.enx3bt45p649},
  note={Transformers for Vision Workshop, CVPR 2023}
}
                </pre>
            </div>
          </td>
        </tr>
        <tr>
          <td width="250px" align="left">
          </td>
          <td width="50px" align="center">
          </td>
          <td width="550px" align="left">
          </td>
        </tr>
      </tbody>
    </table>
    <br>
    <hr>
    <center>
      <h1>Related Projects</h1>
    </center>
    <table align="center" width="900px">
      <tbody>
        <tr>
          <!-- <td align="center" width="300px">
            <a href="https://www.linkedin.com/posts/adityang_cvpr2023-bengaluru-transformersforvision-activity-7076240486130208768-IKbN?utm_source=share&amp;utm_medium=member_desktop">OCTraN</a>
            <center><a href="https://www.linkedin.com/posts/adityang_cvpr2023-bengaluru-transformersforvision-activity-7076240486130208768-IKbN?utm_source=share&amp;utm_medium=member_desktop"><img src="./index_files/paper_OCTraN.png" height="600px"></a><br></center>
          </td> -->
          <td align="center" width="300px">
            <a href="https://www.linkedin.com/posts/adityang_low-cost-hardware-accelerated-vision-based-activity-7067379026075537408-XbFj?utm_source=share&amp;utm_medium=member_desktop">Hardware Accelerated Stereo Vision</a>
            <center><a href="https://www.linkedin.com/posts/adityang_low-cost-hardware-accelerated-vision-based-activity-7067379026075537408-XbFj?utm_source=share&amp;utm_medium=member_desktop"><img src="./index_files/paper_stereo.png" height="600px"></a><br></center>
          </td>
        </tr>
      </tbody>
    </table>
    <br>
    <hr>
    <table align="center" width="800px">
      <tbody>
        <tr>
          <td width="800px">
            <left>
              <center>
                <h1>Say Hi!</h1>
              </center>
              Contact me at <a href="mailto:adityang5@gmail.com">adityang5@gmail.com</a>
              <br>
            </left>
          </td>
        </tr>
      </tbody>
    </table>
    <br><br>
    <script xml:space="preserve" language="JavaScript">
      hideallbibs();
    </script>
  </body>
</html>